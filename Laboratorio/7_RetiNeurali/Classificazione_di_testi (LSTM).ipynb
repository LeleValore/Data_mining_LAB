{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install portalocker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJYMP8_7baum",
        "outputId": "b62d0169-399b-404c-8a51-5e8130816312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from   torchtext.datasets        import AG_NEWS\n",
        "from   torchtext.data.functional import to_map_style_dataset\n",
        "import matplotlib.pyplot         as plt\n",
        "import seaborn                   as sbn\n",
        "\n",
        "# IMPORT NECESSARI PER CREARE IL VOCABOLARIO\n",
        "from torchtext.data              import get_tokenizer\n",
        "from torchtext.vocab             import build_vocab_from_iterator\n",
        "\n",
        "from torch.utils.data            import DataLoader\n",
        "from torch                       import nn\n",
        "from torch.nn                    import functional as F\n",
        "\n",
        "# PER IL CALCOLO DELLA LOSS E DELL'ACCURACY\n",
        "from sklearn.metrics            import accuracy_score\n",
        "from tqdm                       import tqdm\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "eW5NENH4gTS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lettura dati dal dataset AG_NEWS\n",
        "\n",
        "Tale dataset contiene una collezione di articoli generati da 2000 news in più di una anno di attività. Esso è stato definito per scopi di ricerca in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity.\n",
        "\n",
        "Le categorie presenti in tale dataset sono:\n",
        " - World\n",
        " - Sports\n",
        " - Business\n",
        " - Sci/Tech"
      ],
      "metadata": {
        "id": "63FEkO1pD4VH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTA\n",
        "# to_map_style_dataset: converte iterable-style dataset a map-style dataset.\n",
        "agnews_train, agnews_test = AG_NEWS(split=('train', 'test'))\n",
        "agnews_train, agnews_test = to_map_style_dataset(agnews_train), to_map_style_dataset(agnews_test)"
      ],
      "metadata": {
        "id": "75N7KYTOgUZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contiamo quanti record ci sono all'interno del train e del test. Inoltre qual'è la frequenza di ogni classe sia nel train che nel test**"
      ],
      "metadata": {
        "id": "dbyuuas9HwrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categories =  [\"World\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "\n",
        "# TRAIN\n",
        "train_labels       = [label for label, _ in agnews_train]\n",
        "train_labels_nodup = set(train_labels)\n",
        "\n",
        "# TEST\n",
        "test_labels       = [label for label, _ in agnews_test]\n",
        "test_labels_nodup = set(test_labels)"
      ],
      "metadata": {
        "id": "a_11nbWQhI8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Domensione del train: \", len(train_labels))\n",
        "print(\"Domensione del test: \" , len(test_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZnudumfbMYI",
        "outputId": "7f243137-6e51-4ef1-9c03-83d953afc924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Domensione del train:  120000\n",
            "Domensione del test:  7600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def freq_computing(labels_vet):\n",
        "  labels_freq = {}\n",
        "  for label in labels_vet:\n",
        "    if label not in labels_freq:\n",
        "      labels_freq [label] = 0\n",
        "    labels_freq [label] += 1\n",
        "  return labels_freq\n",
        "\n",
        "train_labels_freq = freq_computing(train_labels)\n",
        "test_labels_freq  = freq_computing(test_labels)\n",
        "\n",
        "print(\"Frequenza etichette del train: \", train_labels_freq)\n",
        "print(\"Frequenza etichette del test: \" , test_labels_freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4Vnj-w1b4MG",
        "outputId": "daede6c1-5abc-4925-f7ad-01ff6f304fe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequenza etichette del train:  {3: 30000, 4: 30000, 2: 30000, 1: 30000}\n",
            "Frequenza etichette del test:  {3: 1900, 4: 1900, 2: 1900, 1: 1900}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ciò che faremo adesso è definire il tokenizer (splitta testi in words), e successivamente implementeremo una funzione che, a partire dai testi di train e test crea il vocabolario di termini.\n",
        "\n",
        "Nota:\n",
        " - min_freq: specifica che soltanto le parole con frequenza > 1 verranno mantenute\n",
        " - specials: usato per associare il simbolo indicato nel caso in cui l'elemento non è presente"
      ],
      "metadata": {
        "id": "8IfgHbP-gGX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def vocabulary_creation(dataset_list):\n",
        "  for dataset in dataset_list:\n",
        "    for _, text in dataset:\n",
        "      yield tokenizer(text)\n",
        "\n",
        "datasets = [agnews_train, agnews_test]\n",
        "vocab    = build_vocab_from_iterator(\n",
        "    vocabulary_creation(datasets),\n",
        "    min_freq = 1,\n",
        "    specials = [\"<UNK>\"]\n",
        ")\n",
        "\n",
        "vocab.set_default_index(vocab[\"<UNK>\"])\n",
        "print(\"Dimensione del dizionario: \", len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkhK9EOge3nh",
        "outputId": "3b36d5e1-6961-407a-e7b0-a5754304f37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensione del dizionario:  98635\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Esempio di tockenizer\n",
        "tokens  = tokenizer(\"Hello how are you?, Welcome to Google Colaboratory\")\n",
        "indexes = vocab(tokens)\n",
        "\n",
        "print(\"Tokens: \", tokens)\n",
        "print(\"Indexes: \", indexes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N0o_nMJmwx_",
        "outputId": "fd490616-675f-4027-a9ea-f929523608a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  ['hello', 'how', 'are', 'you', '?', ',', 'welcome', 'to', 'google', 'colaboratory']\n",
            "Indexes:  [12388, 355, 42, 164, 80, 3, 3298, 4, 202, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INIZIO\n",
        "\n",
        "CASO 1:\n",
        "Trasformiamo i vari record deldataset in un vettore di 25 token indicizzati tramite il vocabilario creato.\n",
        "Pertanto:\n",
        " - Se il record ha 25 token, la funzione si occupa di trasformare il vettore di token in un vettore di indici (presenti nel vocabolario)\n",
        " - Se ne ha più di 25, fa un troncamento a 25 e poi effettua l'operazione fatta nel punto precedente.\n",
        " - Se ne ha meno, allunga il vettore a 25 (inserendo zeri) e poi fa quanto detto nel primo punto."
      ],
      "metadata": {
        "id": "Gag9K0cmpe5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Esperimento 1\n",
        "max_words = 25\n",
        "\n",
        "def vectorize_batch(batch, max_words):\n",
        "    # NOTA:\n",
        "    # batch            : [(c1, t1), (c2, t2), (c1, t3)]\n",
        "    # list(zip(*batch)): [(c1, c2, c1), (t1, t2, t3)]\n",
        "    Y, X = list(zip(*batch))\n",
        "    X = [vocab(tokenizer(text)) for text in X] ## Tokenize and map tokens to indexes\n",
        "    X = [\n",
        "           tokens + ([0]* (max_words-len(tokens))) if len(tokens)<max_words else\n",
        "           tokens[:max_words] for tokens in X\n",
        "        ]\n",
        "\n",
        "    # Il -1 sta ad indicare che le label partiranno da 0 a e non da 1 a\n",
        "    return torch.tensor(X, dtype=torch.int32), torch.tensor(Y) - 1\n",
        "\n",
        "\n",
        "train_loader = DataLoader(agnews_train, batch_size=1024, collate_fn=lambda batch: vectorize_batch(batch, max_words), shuffle=True, pin_memory=True)\n",
        "test_loader  = DataLoader(agnews_test , batch_size=1024, collate_fn=lambda batch: vectorize_batch(batch, max_words), pin_memory=True)"
      ],
      "metadata": {
        "id": "3MqHML2H6_oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CREAZIONE DELLA RETE\n",
        "\n",
        " - Embedding Layer:\n",
        "L' Embedding Layer prende in ingresso il vocabolario di termini, ed associa un vettore di numeri di lunghezza pari a embedding_lenght.\n",
        "Quando passeremo il batch (batch_size, max_tokens) in ingresso all' embedding layer, otterremo in uscita un tensore di dimensioni (batch_size, max_tokens, embed_len)  Ad ogni token sarà associato il rispettivo embedding vector.\n",
        "\n",
        " - LSTM Layer\n",
        "Prende in imput gli embedding generati dal livello precedente. Quindi, l' input di tale livello è (batch_size, max_tokens, embed_len), mentre l'output sarà (batch_size, max_tokens, hidden_dim).\n"
      ],
      "metadata": {
        "id": "8EvXqjQpBx8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_len  = 50\n",
        "hidden_dim = 75\n",
        "n_layers   = 1\n",
        "device     = torch.device('cuda')\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, device):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.device          = device\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embed_len)\n",
        "        self.lstm            = nn.LSTM(input_size=embed_len, hidden_size=hidden_dim, num_layers=n_layers, batch_first=True)\n",
        "        self.linear          = nn.Linear(hidden_dim, len(categories))\n",
        "\n",
        "    def forward(self, X_batch):\n",
        "        embeddings           = self.embedding_layer(X_batch)\n",
        "        hidden_carry         = self.init_hidden_carry(self.device, X_batch)\n",
        "        output, hidden_carry = self.lstm(embeddings, hidden_carry)\n",
        "        return self.linear(output[:,-1])\n",
        "\n",
        "    def init_hidden_carry(self, device, X_batch):\n",
        "      hidden = torch.randn(n_layers, len(X_batch), hidden_dim)\n",
        "      carry  = torch.randn(n_layers, len(X_batch), hidden_dim)\n",
        "      hidden = hidden.to(device, non_blocking=True)\n",
        "      carry  = carry.to(device,  non_blocking=True)\n",
        "      return (hidden, carry)\n",
        "\n",
        "lstm_classifier = LSTMClassifier(device)\n",
        "lstm_classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jN5gpiFwJtE4",
        "outputId": "27cc8b5a-66e4-42ff-da1e-d202e1534609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMClassifier(\n",
              "  (embedding_layer): Embedding(98635, 50)\n",
              "  (lstm): LSTM(50, 75, batch_first=True)\n",
              "  (linear): Linear(in_features=75, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Definizione di funzioni che verranno utilizzati per valutare la bontà del modello (loss e accuracy)**"
      ],
      "metadata": {
        "id": "yeeTL-2PU5Mb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CalcValLossAndAccuracy(model, loss_fn, val_loader):\n",
        "    # with torch.no_grad():\n",
        "    Y_shuffled, Y_preds, losses = [],[],[]\n",
        "    for X, Y in val_loader:\n",
        "        preds = model(X)\n",
        "        loss = loss_fn(preds, Y)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        Y_shuffled.append(Y)\n",
        "        Y_preds.append(preds.argmax(dim=-1))\n",
        "\n",
        "    Y_shuffled = torch.cat(Y_shuffled)\n",
        "    Y_preds    = torch.cat(Y_preds)\n",
        "\n",
        "    print(\"Valid Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "    accuracy = torch.tensor(torch.sum(Y_preds == Y_shuffled).item() / len(Y_preds))\n",
        "    print(\"Valid Acc  : {:.3f}\".format(accuracy))\n",
        "\n",
        "\n",
        "def TrainModel(model, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
        "    for i in range(1, epochs+1):\n",
        "        losses = []\n",
        "        for X, Y in tqdm(train_loader):\n",
        "            Y_preds = model(X)\n",
        "            loss    = loss_fn(Y_preds, Y) ## Calculate Loss\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            optimizer.zero_grad()         ## Clear previously calculated gradients\n",
        "            loss.backward()               ## Calcola il gradiente\n",
        "            optimizer.step()              ## Aggiorna i pesi della rete.\n",
        "\n",
        "        print(\"Train Loss : {:.3f}\".format(torch.tensor(losses).mean()))\n",
        "        CalcValLossAndAccuracy(model, loss_fn, val_loader)"
      ],
      "metadata": {
        "id": "yJFCTkglVE6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spostiamo i dati sulla GPU**"
      ],
      "metadata": {
        "id": "xWLoSmXyP_MS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_device(data, device):\n",
        "    # Sposta i tensori sul device selezionato\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    # Dataloader per caricare i dati sul device\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl     = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        # Sposta il batch sul device\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Numero di batch\n",
        "        return len(self.dl)\n",
        "\n",
        "train_loader    = DeviceDataLoader(train_loader, device)\n",
        "test_loader     = DeviceDataLoader(test_loader,  device)\n",
        "lstm_classifier = to_device(lstm_classifier, device);"
      ],
      "metadata": {
        "id": "vb8iMtkTO5UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Addestriamo**"
      ],
      "metadata": {
        "id": "0MrBLDeQQ25q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "epochs        = 10\n",
        "learning_rate = 1e-3\n",
        "loss_fn       = nn.CrossEntropyLoss()\n",
        "optimizer     = Adam(lstm_classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "TrainModel(lstm_classifier, loss_fn, optimizer, train_loader, test_loader, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XycEYFWQWD_6",
        "outputId": "3a9904e9-6b9f-49bf-a2a7-17726efe1e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 19.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.522\n",
            "Valid Loss : 0.503\n",
            "Valid Acc  : 0.816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 23.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.432\n",
            "Valid Loss : 0.453\n",
            "Valid Acc  : 0.837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 23.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.376\n",
            "Valid Loss : 0.419\n",
            "Valid Acc  : 0.854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.332\n",
            "Valid Loss : 0.396\n",
            "Valid Acc  : 0.860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 23.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.300\n",
            "Valid Loss : 0.385\n",
            "Valid Acc  : 0.868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.271\n",
            "Valid Loss : 0.366\n",
            "Valid Acc  : 0.869\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.246\n",
            "Valid Loss : 0.363\n",
            "Valid Acc  : 0.873\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 19.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.226\n",
            "Valid Loss : 0.362\n",
            "Valid Acc  : 0.875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:04<00:00, 24.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.206\n",
            "Valid Loss : 0.363\n",
            "Valid Acc  : 0.878\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.188\n",
            "Valid Loss : 0.359\n",
            "Valid Acc  : 0.882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def MakePredictions(model, loader):\n",
        "    Y_shuffled, Y_preds = [], []\n",
        "    for X, Y in loader:\n",
        "        preds = model(X)\n",
        "        Y_preds.append(preds)\n",
        "        Y_shuffled.append(Y)\n",
        "\n",
        "\n",
        "    Y_preds, Y_shuffled = torch.cat(Y_preds), torch.cat(Y_shuffled)\n",
        "    return Y_shuffled.detach().cpu().numpy(), Y_preds.argmax(dim=-1).detach().cpu().numpy()\n",
        "\n",
        "Y_actual, Y_preds = MakePredictions(lstm_classifier, test_loader)"
      ],
      "metadata": {
        "id": "gwVjmgcEZMfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Test Accuracy : {}\".format(accuracy_score(Y_actual, Y_preds)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds, target_names=categories))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOQUz6TaaYjy",
        "outputId": "889959b8-87ae-425b-f37a-0b5d50343cce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy : 0.8825\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.91      0.88      0.89      1900\n",
            "      Sports       0.93      0.95      0.94      1900\n",
            "    Business       0.86      0.83      0.84      1900\n",
            "    Sci/Tech       0.84      0.87      0.85      1900\n",
            "\n",
            "    accuracy                           0.88      7600\n",
            "   macro avg       0.88      0.88      0.88      7600\n",
            "weighted avg       0.88      0.88      0.88      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1665   67   79   89]\n",
            " [  31 1812   30   27]\n",
            " [  76   43 1582  199]\n",
            " [  63   32  157 1648]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CASO 2:\n",
        " - max_words = 50"
      ],
      "metadata": {
        "id": "kNTAe7XvTiM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_words    = 50\n",
        "\n",
        "train_loader = DataLoader(agnews_train, batch_size=1024, collate_fn=lambda batch: vectorize_batch(batch, max_words), shuffle=True, pin_memory=True)\n",
        "test_loader  = DataLoader(agnews_test , batch_size=1024, collate_fn=lambda batch: vectorize_batch(batch, max_words), pin_memory=True)"
      ],
      "metadata": {
        "id": "wN5EbyWjThug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader    = DeviceDataLoader(train_loader, device)\n",
        "test_loader     = DeviceDataLoader(test_loader,  device)\n",
        "lstm_classifier = to_device(lstm_classifier, device);"
      ],
      "metadata": {
        "id": "14w86oX9XwTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs        = 10\n",
        "learning_rate = 1e-3\n",
        "loss_fn       = nn.CrossEntropyLoss()\n",
        "optimizer     = Adam(lstm_classifier.parameters(), lr=learning_rate)\n",
        "\n",
        "TrainModel(lstm_classifier, loss_fn, optimizer, train_loader, test_loader, epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdyPoIIFX3sS",
        "outputId": "f827f085-0787-4c1d-e20c-d7293e966f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.224\n",
            "Valid Loss : 0.346\n",
            "Valid Acc  : 0.894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.181\n",
            "Valid Loss : 0.333\n",
            "Valid Acc  : 0.900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.160\n",
            "Valid Loss : 0.339\n",
            "Valid Acc  : 0.896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.145\n",
            "Valid Loss : 0.335\n",
            "Valid Acc  : 0.899\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.133\n",
            "Valid Loss : 0.326\n",
            "Valid Acc  : 0.902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 17.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.122\n",
            "Valid Loss : 0.339\n",
            "Valid Acc  : 0.900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.112\n",
            "Valid Loss : 0.342\n",
            "Valid Acc  : 0.898\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.106\n",
            "Valid Loss : 0.363\n",
            "Valid Acc  : 0.896\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:05<00:00, 22.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.097\n",
            "Valid Loss : 0.351\n",
            "Valid Acc  : 0.900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 118/118 [00:06<00:00, 18.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss : 0.089\n",
            "Valid Loss : 0.368\n",
            "Valid Acc  : 0.896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y_actual, Y_preds = MakePredictions(lstm_classifier, test_loader)\n",
        "print(\"Test Accuracy : {}\".format(accuracy_score(Y_actual, Y_preds)))\n",
        "print(\"\\nClassification Report : \")\n",
        "print(classification_report(Y_actual, Y_preds, target_names=categories))\n",
        "print(\"\\nConfusion Matrix : \")\n",
        "print(confusion_matrix(Y_actual, Y_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baWWdbplYWM5",
        "outputId": "24f4f113-2cb4-49e0-8551-d8a95975c5d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy : 0.8963157894736842\n",
            "\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       World       0.91      0.90      0.91      1900\n",
            "      Sports       0.96      0.95      0.96      1900\n",
            "    Business       0.84      0.88      0.86      1900\n",
            "    Sci/Tech       0.87      0.86      0.87      1900\n",
            "\n",
            "    accuracy                           0.90      7600\n",
            "   macro avg       0.90      0.90      0.90      7600\n",
            "weighted avg       0.90      0.90      0.90      7600\n",
            "\n",
            "\n",
            "Confusion Matrix : \n",
            "[[1708   33   79   80]\n",
            " [  42 1802   36   20]\n",
            " [  71   19 1673  137]\n",
            " [  48   14  209 1629]]\n"
          ]
        }
      ]
    }
  ]
}